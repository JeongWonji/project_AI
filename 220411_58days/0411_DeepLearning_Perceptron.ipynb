{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3708cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value : 0.7578555345535278\n",
      "loss value : 0.6931946277618408\n",
      "loss value : 0.6931514739990234\n",
      "loss value : 0.693147599697113\n",
      "loss value : 0.6931471824645996\n",
      "loss value : 0.6931471824645996\n",
      "loss value : 0.6931471824645996\n",
      "loss value : 0.6931471824645996\n",
      "loss value : 0.6931471824645996\n",
      "loss value : 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression으로 GATE연산을 학습할 수 있는지 확인!\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=np.float64)\n",
    "\n",
    "# AND GATE 연산에 대한 t_data\n",
    "# t_data = np.array([0, 0, 0, 1], dtype=np.float64)\n",
    "\n",
    "# OR GATE 연산에 대한 t_data\n",
    "# t_data = np.array([0, 1, 1, 1], dtype=np.float64)\n",
    "\n",
    "# XOR GATE 연산에 대한 t_data\n",
    "t_data = np.array([0, 1, 1, 0], dtype=np.float64)\n",
    "\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([2,1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "# Hypothesis, model\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# Session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복학습\n",
    "for step in range(30000):\n",
    "    _, loss_val = sess.run([train, loss],\n",
    "                           feed_dict={X:x_data,\n",
    "                                      T:t_data.reshape(-1,1)})\n",
    "    if step % 3000 == 0:\n",
    "        print('loss value : {}'.format(loss_val))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f6f0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.50      0.67         2\n",
      "         1.0       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.75      0.73         4\n",
      "weighted avg       0.83      0.75      0.73         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation(모델평가)\n",
    "predict = tf.cast(H >= 0.5, dtype=tf.float32)\n",
    "predict_val = sess.run(predict, feed_dict={X:x_data})\n",
    "print(predict_val)\n",
    "# \n",
    "print(classification_report(t_data, predict_val.ravel()))\n",
    "# 결과를 확인해서 logistic regression이 진리표를 학습할 수 있느지를 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7d8378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value : 1.0286104679107666\n",
      "loss value : 0.6785933375358582\n",
      "loss value : 0.6420406103134155\n",
      "loss value : 0.5540069341659546\n",
      "loss value : 0.36861780285835266\n",
      "loss value : 0.16834038496017456\n",
      "loss value : 0.07950637489557266\n",
      "loss value : 0.0462171770632267\n",
      "loss value : 0.031088195741176605\n",
      "loss value : 0.022914141416549683\n"
     ]
    }
   ],
   "source": [
    "# DNN으로 XOR GATE연산을 학습할 수 있는지 확인!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=np.float64)\n",
    "\n",
    "# XOR GATE 연산에 대한 t_data\n",
    "t_data = np.array([0, 1, 1, 0], dtype=np.float64)\n",
    "\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W2 = tf.Variable(tf.random.normal([2,10]))\n",
    "b2 = tf.Variable(tf.random.normal([10]))\n",
    "layer2 = tf.sigmoid(tf.matmul(X,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([10,6]))\n",
    "b3 = tf.Variable(tf.random.normal([6]))\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([6,1]))\n",
    "b4 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "\n",
    "# Hypothesis, model\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# Session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복학습\n",
    "for step in range(30000):\n",
    "    _, loss_val = sess.run([train, loss],\n",
    "                           feed_dict={X:x_data,\n",
    "                                      T:t_data.reshape(-1,1)})\n",
    "    if step % 3000 == 0:\n",
    "        print('loss value : {}'.format(loss_val))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68bb6398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         2\n",
      "         1.0       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation(모델평가)\n",
    "predict = tf.cast(H >= 0.5, dtype=tf.float32)\n",
    "predict_val = sess.run(predict, feed_dict={X:x_data})\n",
    "print(predict_val)\n",
    "# \n",
    "print(classification_report(t_data, predict_val.ravel()))\n",
    "# 결과를 확인해서 logistic regression이 진리표를 학습할 수 있느지를 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42933f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f485acac10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorflow 2.x 버전으로 구현해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=np.float64)\n",
    "\n",
    "# XOR GATE 연산에 대한 t_data\n",
    "t_data = np.array([0, 1, 1, 0], dtype=np.float64)\n",
    "\n",
    "# Tensorflow 구현\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(2,)))     # input layer\n",
    "model.add(Dense(units=128,\n",
    "                activation='sigmoid'))   # 1번째 hidden layer\n",
    "model.add(Dense(units=32,\n",
    "                activation='sigmoid'))   # 2번째 hidden layer\n",
    "model.add(Dense(units=16,\n",
    "                activation='sigmoid'))   # 3번째 hidden layer\n",
    "model.add(Dense(units=1,\n",
    "                activation='sigmoid'))   # output layer\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-2),\n",
    "              loss='binary_crossentropy')\n",
    "\n",
    "model.fit(x_data,\n",
    "          t_data.reshape(-1,1),\n",
    "          epochs=30000,\n",
    "          verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78bb4193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F48615A550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.50      0.50         2\n",
      "         1.0       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.50         4\n",
      "   macro avg       0.50      0.50      0.50         4\n",
      "weighted avg       0.50      0.50      0.50         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation(모델평가)\n",
    "predict_val = model.predict(x_data)\n",
    "predict_val = (tf.cast(predict_val > 0.5, dtype=tf.float32)).numpy().ravel()\n",
    "\n",
    "print(classification_report(t_data, predict_val))\n",
    "# 파라미터 조절을 더 해야 할 듯 해요!!!\n",
    "# 정상적으로 학습이 진행되어야 합니다.!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_TF2]",
   "language": "python",
   "name": "conda-env-machine_TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
