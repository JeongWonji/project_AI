{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb5d4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MNIST 예제를 구현해 보아요!\n",
    "# Data는 Kaggle에서 다운로드 할꺼예요!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "display(df.shape)   # (42000, 785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "# 결측치나 이상치가 존재하지 않아요!\n",
    "# 단, 정규화는 필요해요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1b7adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이미지 확인\n",
    "figure = plt.figure()\n",
    "ax_arr = []  # python list\n",
    "\n",
    "img_data = df.drop('label', axis=1, inplace=False).values\n",
    "\n",
    "for n in range(10):\n",
    "    ax_arr.append(figure.add_subplot(2,5,n+1))\n",
    "    ax_arr[n].imshow(img_data[n].reshape(28,28), \n",
    "                     cmap='Greys',            # 흑백이미지 표현\n",
    "                     interpolation='nearest') # 보간법\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41be6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8397a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\campusseven02\\AppData\\Local\\Temp\\ipykernel_4500\\2739525339.py:2: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\campusseven02\\AppData\\Local\\Temp\\ipykernel_4500\\2739525339.py:8: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\campusseven02\\AppData\\Local\\Temp\\ipykernel_4500\\2739525339.py:24: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\campusseven02\\AppData\\Local\\Temp\\ipykernel_4500\\2739525339.py:27: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "loss val : 1.5307165384292603\n",
      "loss val : 0.20722626149654388\n",
      "loss val : 0.17508423328399658\n",
      "loss val : 0.17157819867134094\n",
      "loss val : 0.17306602001190186\n",
      "loss val : 0.17516613006591797\n",
      "loss val : 0.1768408566713333\n",
      "loss val : 0.17799437046051025\n",
      "loss val : 0.17876680195331573\n",
      "loss val : 0.17928001284599304\n"
     ]
    }
   ],
   "source": [
    "## Tensorflow Implementation ##\n",
    "sess = tf.Session()\n",
    "\n",
    "onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=10))\n",
    "onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=10))\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([784,10]))\n",
    "b = tf.Variable(tf.random.normal([10]))\n",
    "\n",
    "# Hypothesis, Model\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,\n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복학습\n",
    "num_of_epoch = 1000\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    \n",
    "    total_batch = int(norm_train_x_data.shape[0] / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_x = norm_train_x_data[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y = onehot_train_t_data[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        _, loss_val = sess.run([train, loss], feed_dict={X:batch_x,\n",
    "                                                         T:batch_y})\n",
    "    if step % 100 == 0:\n",
    "        print('loss val : {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6a44a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9077777862548828\n"
     ]
    }
   ],
   "source": [
    "# accuracy 측정\n",
    "\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "accuracy_val = sess.run(accuracy, feed_dict={X:norm_test_x_data,\n",
    "                                             T:onehot_test_t_data})\n",
    "print('Accuracy : {}'.format(accuracy_val))  # 0.9077777862548828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc6f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[1.1614635]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)   # 2.3.0\n",
    "\n",
    "W = tf.random.normal([1], dtype=tf.float32)\n",
    "\n",
    "# 1.15버전에서 W의 값을 알아내려면 session을 통해서 node를 실행시켜서\n",
    "# 값을 얻어야 해요!\n",
    "# 2.x 버전은 eager execution(즉시실행모드)을 지원해요.\n",
    "# session이 필요가 없고 일반적인 프로그래밍 하는 것처럼 사용할 수 있어요!\n",
    "print(W.numpy())   # [-0.7210334]\n",
    "\n",
    "# 추가적으로 초기화 하는 코드 역시 불필요해서 이제는 사용하지 않아요!\n",
    "# sess.run(tf.gloabal_variables_initializer())  # 사용하지 않아요!\n",
    "\n",
    "# placeholder도 역시 삭제되었어요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febf7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그러면 Keras를 사용한다고 하는데..\n",
    "# 코드는 어떻게 작성하는 건가요??\n",
    "\n",
    "# 그림과 매칭해서 봐야 해요!!\n",
    "\n",
    "# keras의 model은 어떻게 만드나요?\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# model.add()를 이용해서 layer를 추가해 줘요!\n",
    "# model.add('input layer')\n",
    "# model.add('output layter')\n",
    "\n",
    "# loss의 종류와 optimizer종류를 설정\n",
    "# model.compile()\n",
    "\n",
    "# 학습 (마치 sklearn 사용하는 것처럼..)\n",
    "# model.fit()\n",
    "\n",
    "# 평가와 predict\n",
    "# model.evaluate()  => 모델평가\n",
    "# model.predict()   => 예측값 도출\n",
    "\n",
    "# 모델 저장\n",
    "# model.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aaa3f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reset\n",
    "# 대표적인 multinomial 예제인 MNIST를 이용해서 \n",
    "# Tensorflow 2.x 버전으로 구현해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # keras model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Flatten(Input Layer)\n",
    "                                                    # Dense(Output Layer)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Raw Data Loading\n",
    "\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d471d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ae2e9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 2.x 구현\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()\n",
    "\n",
    "# layer 추가\n",
    "# input layer\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "# input layer는 사실 하는일 없어요! 그래서 코드를 나눠쓰지 않고 한번에\n",
    "# 기술할 수 도 있어요! (나중에 해요~)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c015b42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 2.3176 - accuracy: 0.1301WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 2.2969 - accuracy: 0.1436 - val_loss: 2.1717 - val_accuracy: 0.2175\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 2.0554 - accuracy: 0.3230 - val_loss: 1.9593 - val_accuracy: 0.3974\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.8638 - accuracy: 0.4791 - val_loss: 1.7846 - val_accuracy: 0.5272\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.7049 - accuracy: 0.5807 - val_loss: 1.6387 - val_accuracy: 0.6097\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.5723 - accuracy: 0.6435 - val_loss: 1.5168 - val_accuracy: 0.6633\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.4611 - accuracy: 0.6860 - val_loss: 1.4142 - val_accuracy: 0.7007\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.3673 - accuracy: 0.7163 - val_loss: 1.3276 - val_accuracy: 0.7259\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.2877 - accuracy: 0.7359 - val_loss: 1.2535 - val_accuracy: 0.7412\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.2194 - accuracy: 0.7523 - val_loss: 1.1899 - val_accuracy: 0.7543\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1604 - accuracy: 0.7653 - val_loss: 1.1347 - val_accuracy: 0.7663\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1090 - accuracy: 0.7756 - val_loss: 1.0865 - val_accuracy: 0.7782\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0640 - accuracy: 0.7850 - val_loss: 1.0440 - val_accuracy: 0.7895\n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0241 - accuracy: 0.7920 - val_loss: 1.0063 - val_accuracy: 0.7964\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9886 - accuracy: 0.7974 - val_loss: 0.9728 - val_accuracy: 0.8032\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9569 - accuracy: 0.8030 - val_loss: 0.9426 - val_accuracy: 0.8066\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9282 - accuracy: 0.8070 - val_loss: 0.9154 - val_accuracy: 0.8116\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9023 - accuracy: 0.8126 - val_loss: 0.8906 - val_accuracy: 0.8162\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8787 - accuracy: 0.8162 - val_loss: 0.8681 - val_accuracy: 0.8202\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8572 - accuracy: 0.8199 - val_loss: 0.8474 - val_accuracy: 0.8240\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8374 - accuracy: 0.8230 - val_loss: 0.8285 - val_accuracy: 0.8274\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8192 - accuracy: 0.8261 - val_loss: 0.8110 - val_accuracy: 0.8294\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8023 - accuracy: 0.8292 - val_loss: 0.7949 - val_accuracy: 0.8318\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7867 - accuracy: 0.8313 - val_loss: 0.7799 - val_accuracy: 0.8349\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7722 - accuracy: 0.8336 - val_loss: 0.7659 - val_accuracy: 0.8361\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7586 - accuracy: 0.8354 - val_loss: 0.7528 - val_accuracy: 0.8381\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7459 - accuracy: 0.8376 - val_loss: 0.7406 - val_accuracy: 0.8413\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.7340 - accuracy: 0.8390 - val_loss: 0.7291 - val_accuracy: 0.8434\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7228 - accuracy: 0.8408 - val_loss: 0.7183 - val_accuracy: 0.8451\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7123 - accuracy: 0.8426 - val_loss: 0.7082 - val_accuracy: 0.8468\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7023 - accuracy: 0.8438 - val_loss: 0.6986 - val_accuracy: 0.8480\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.8457 - val_loss: 0.6895 - val_accuracy: 0.8490\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6840 - accuracy: 0.8466 - val_loss: 0.6810 - val_accuracy: 0.8502\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6756 - accuracy: 0.8485 - val_loss: 0.6728 - val_accuracy: 0.8512\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6676 - accuracy: 0.8494 - val_loss: 0.6650 - val_accuracy: 0.8515\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6599 - accuracy: 0.8507 - val_loss: 0.6576 - val_accuracy: 0.8522\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6526 - accuracy: 0.8517 - val_loss: 0.6507 - val_accuracy: 0.8532\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6457 - accuracy: 0.8528 - val_loss: 0.6439 - val_accuracy: 0.8536\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6390 - accuracy: 0.8540 - val_loss: 0.6375 - val_accuracy: 0.8541\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6327 - accuracy: 0.8546 - val_loss: 0.6314 - val_accuracy: 0.8551\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6266 - accuracy: 0.8558 - val_loss: 0.6255 - val_accuracy: 0.8558\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6208 - accuracy: 0.8568 - val_loss: 0.6199 - val_accuracy: 0.8568\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6152 - accuracy: 0.8574 - val_loss: 0.6145 - val_accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6098 - accuracy: 0.8582 - val_loss: 0.6094 - val_accuracy: 0.8580\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6046 - accuracy: 0.8590 - val_loss: 0.6043 - val_accuracy: 0.8580\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5997 - accuracy: 0.8597 - val_loss: 0.5995 - val_accuracy: 0.8588\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5949 - accuracy: 0.8602 - val_loss: 0.5949 - val_accuracy: 0.8595\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5903 - accuracy: 0.8609 - val_loss: 0.5904 - val_accuracy: 0.8605\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5858 - accuracy: 0.8616 - val_loss: 0.5861 - val_accuracy: 0.8612\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5815 - accuracy: 0.8625 - val_loss: 0.5820 - val_accuracy: 0.8616\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5773 - accuracy: 0.8629 - val_loss: 0.5780 - val_accuracy: 0.8624\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5733 - accuracy: 0.8638 - val_loss: 0.5741 - val_accuracy: 0.8629\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5694 - accuracy: 0.8648 - val_loss: 0.5704 - val_accuracy: 0.8636\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5656 - accuracy: 0.8651 - val_loss: 0.5667 - val_accuracy: 0.8643\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5620 - accuracy: 0.8659 - val_loss: 0.5632 - val_accuracy: 0.8655\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.8662 - val_loss: 0.5598 - val_accuracy: 0.8656\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5550 - accuracy: 0.8668 - val_loss: 0.5564 - val_accuracy: 0.8662\n",
      "Epoch 57/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5517 - accuracy: 0.8673 - val_loss: 0.5532 - val_accuracy: 0.8668\n",
      "Epoch 58/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5484 - accuracy: 0.8681 - val_loss: 0.5501 - val_accuracy: 0.8673\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5453 - accuracy: 0.8687 - val_loss: 0.5471 - val_accuracy: 0.8677\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5422 - accuracy: 0.8692 - val_loss: 0.5441 - val_accuracy: 0.8679\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5392 - accuracy: 0.8691 - val_loss: 0.5413 - val_accuracy: 0.8679\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5363 - accuracy: 0.8699 - val_loss: 0.5385 - val_accuracy: 0.8684\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5335 - accuracy: 0.8704 - val_loss: 0.5357 - val_accuracy: 0.8685\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5308 - accuracy: 0.8707 - val_loss: 0.5331 - val_accuracy: 0.8687\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5281 - accuracy: 0.8713 - val_loss: 0.5306 - val_accuracy: 0.8690\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5255 - accuracy: 0.8718 - val_loss: 0.5281 - val_accuracy: 0.8697\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5230 - accuracy: 0.8719 - val_loss: 0.5256 - val_accuracy: 0.8702\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5205 - accuracy: 0.8724 - val_loss: 0.5233 - val_accuracy: 0.8709\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5181 - accuracy: 0.8727 - val_loss: 0.5210 - val_accuracy: 0.8713\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5157 - accuracy: 0.8731 - val_loss: 0.5187 - val_accuracy: 0.8714\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5134 - accuracy: 0.8734 - val_loss: 0.5165 - val_accuracy: 0.8716\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5112 - accuracy: 0.8738 - val_loss: 0.5143 - val_accuracy: 0.8719\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5090 - accuracy: 0.8740 - val_loss: 0.5122 - val_accuracy: 0.8721\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5068 - accuracy: 0.8741 - val_loss: 0.5101 - val_accuracy: 0.8726\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5047 - accuracy: 0.8741 - val_loss: 0.5081 - val_accuracy: 0.8724\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5026 - accuracy: 0.8745 - val_loss: 0.5061 - val_accuracy: 0.8726\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.8748 - val_loss: 0.5042 - val_accuracy: 0.8731\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4986 - accuracy: 0.8753 - val_loss: 0.5023 - val_accuracy: 0.8731\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4967 - accuracy: 0.8756 - val_loss: 0.5005 - val_accuracy: 0.8735\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4948 - accuracy: 0.8756 - val_loss: 0.4987 - val_accuracy: 0.8738\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4930 - accuracy: 0.8764 - val_loss: 0.4969 - val_accuracy: 0.8735\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4911 - accuracy: 0.8767 - val_loss: 0.4951 - val_accuracy: 0.8740\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4894 - accuracy: 0.8769 - val_loss: 0.4934 - val_accuracy: 0.8741\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4876 - accuracy: 0.8775 - val_loss: 0.4918 - val_accuracy: 0.8743\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4859 - accuracy: 0.8772 - val_loss: 0.4901 - val_accuracy: 0.8747\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4842 - accuracy: 0.8778 - val_loss: 0.4885 - val_accuracy: 0.8753\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4826 - accuracy: 0.8778 - val_loss: 0.4869 - val_accuracy: 0.8760\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4810 - accuracy: 0.8778 - val_loss: 0.4854 - val_accuracy: 0.8767\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4794 - accuracy: 0.8784 - val_loss: 0.4839 - val_accuracy: 0.8770\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4778 - accuracy: 0.8786 - val_loss: 0.4824 - val_accuracy: 0.8774\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.8786 - val_loss: 0.4809 - val_accuracy: 0.8772\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4748 - accuracy: 0.8789 - val_loss: 0.4795 - val_accuracy: 0.8779\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4733 - accuracy: 0.8790 - val_loss: 0.4781 - val_accuracy: 0.8777\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4719 - accuracy: 0.8794 - val_loss: 0.4767 - val_accuracy: 0.8779\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4704 - accuracy: 0.8797 - val_loss: 0.4754 - val_accuracy: 0.8777\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4690 - accuracy: 0.8803 - val_loss: 0.4740 - val_accuracy: 0.8784\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4677 - accuracy: 0.8803 - val_loss: 0.4727 - val_accuracy: 0.8784\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4663 - accuracy: 0.8805 - val_loss: 0.4714 - val_accuracy: 0.8782\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4650 - accuracy: 0.8808 - val_loss: 0.4701 - val_accuracy: 0.8781\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.8808 - val_loss: 0.4689 - val_accuracy: 0.8784\n"
     ]
    }
   ],
   "source": [
    "# model compile\n",
    "# 사용할 loss 함수를 지정, 사용한 optimizer(알고리즘)를 지정\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# loss\n",
    "# linear regression : linear\n",
    "# binary classification : binary_crossentropy\n",
    "# multinomial classification : categorical_crossentropy(onehot encoding처리를 해야 해요!)\n",
    "# multinomial classification : sparse_categorical_crossentropy(onehot처리가 필요 없어요!)\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 학습결과를 변수에 저장\n",
    "history = model.fit(norm_train_x_data,\n",
    "                    train_t_data,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb1e9733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 0s 660us/step - loss: 0.4799 - accuracy: 0.8760\n",
      "[0.4799305200576782, 0.876031756401062]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#        loss               accuracy\n",
    "# [0.4799305200576782, 0.876031756401062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 만든 모델을 저장해 보아요!\n",
    "# 학습 한 후 모델이 메모리에 저장되 있어요. 프로그램 종료하면 다 날라가요!\n",
    "# 내일 다시 하려면 처음부터 다시 학습해야해요! => 시간이 오래 걸려요!\n",
    "\n",
    "# 모델학습에 시간이 너무 오래걸리는 경우\n",
    "# 중간에 미리 저장해 놓으면 거기서부터 재 학습이 가능!\n",
    "\n",
    "# 다른사람과 모델 공유가 가능!\n",
    "\n",
    "# 저장을 할 때 2가지 방법이 있어요!\n",
    "# 모델을 저장할 때 모델 구조와 계산된 W,b를 같이 저장할 수 있어요!\n",
    "# 장점 => 편해요!  단점 => 사이즈 커요!\n",
    "\n",
    "# 모델을 저장할 때 모델 구조는 저장하지 않고 W,b만 저장\n",
    "# 장점 => 크기가 작아요. 단점 => 사용하려면 모델을 먼저 만들고 W,b를 로딩.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01793416",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 2.3729 - accuracy: 0.1219WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "\n",
      "Epoch 00001: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 2.3526 - accuracy: 0.1324 - val_loss: 2.2146 - val_accuracy: 0.2031\n",
      "Epoch 2/100\n",
      "202/236 [========================>.....] - ETA: 0s - loss: 2.1257 - accuracy: 0.2803\n",
      "Epoch 00002: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 2.1125 - accuracy: 0.2918 - val_loss: 2.0027 - val_accuracy: 0.3891\n",
      "Epoch 3/100\n",
      "196/236 [=======================>......] - ETA: 0s - loss: 1.9353 - accuracy: 0.4347\n",
      "Epoch 00003: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.9177 - accuracy: 0.4491 - val_loss: 1.8255 - val_accuracy: 0.5133\n",
      "Epoch 4/100\n",
      "188/236 [======================>.......] - ETA: 0s - loss: 1.7697 - accuracy: 0.5475\n",
      "Epoch 00004: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.7544 - accuracy: 0.5559 - val_loss: 1.6768 - val_accuracy: 0.5968\n",
      "Epoch 5/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 1.6296 - accuracy: 0.6187\n",
      "Epoch 00005: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.6173 - accuracy: 0.6247 - val_loss: 1.5516 - val_accuracy: 0.6548\n",
      "Epoch 6/100\n",
      "190/236 [=======================>......] - ETA: 0s - loss: 1.5105 - accuracy: 0.6671\n",
      "Epoch 00006: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.5019 - accuracy: 0.6690 - val_loss: 1.4460 - val_accuracy: 0.6937\n",
      "Epoch 7/100\n",
      "200/236 [========================>.....] - ETA: 0s - loss: 1.4086 - accuracy: 0.7017\n",
      "Epoch 00007: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.4042 - accuracy: 0.7010 - val_loss: 1.3562 - val_accuracy: 0.7180\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 1.3209 - accuracy: 0.7238\n",
      "Epoch 00008: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.3209 - accuracy: 0.7238 - val_loss: 1.2795 - val_accuracy: 0.7372\n",
      "Epoch 9/100\n",
      "193/236 [=======================>......] - ETA: 0s - loss: 1.2560 - accuracy: 0.7406\n",
      "Epoch 00009: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.2495 - accuracy: 0.7425 - val_loss: 1.2134 - val_accuracy: 0.7544\n",
      "Epoch 10/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 1.1936 - accuracy: 0.7570\n",
      "Epoch 00010: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1877 - accuracy: 0.7568 - val_loss: 1.1561 - val_accuracy: 0.7665\n",
      "Epoch 11/100\n",
      "187/236 [======================>.......] - ETA: 0s - loss: 1.1377 - accuracy: 0.7681\n",
      "Epoch 00011: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1340 - accuracy: 0.7683 - val_loss: 1.1060 - val_accuracy: 0.7777\n",
      "Epoch 12/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 1.0879 - accuracy: 0.7777\n",
      "Epoch 00012: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0868 - accuracy: 0.7770 - val_loss: 1.0619 - val_accuracy: 0.7862\n",
      "Epoch 13/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 1.0473 - accuracy: 0.7867\n",
      "Epoch 00013: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0451 - accuracy: 0.7860 - val_loss: 1.0227 - val_accuracy: 0.7937\n",
      "Epoch 14/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 1.0083 - accuracy: 0.7934\n",
      "Epoch 00014: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.0079 - accuracy: 0.7934 - val_loss: 0.9878 - val_accuracy: 0.8007\n",
      "Epoch 15/100\n",
      "192/236 [=======================>......] - ETA: 0s - loss: 0.9775 - accuracy: 0.7967\n",
      "Epoch 00015: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9747 - accuracy: 0.7985 - val_loss: 0.9564 - val_accuracy: 0.8063\n",
      "Epoch 16/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 0.9491 - accuracy: 0.8037\n",
      "Epoch 00016: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9448 - accuracy: 0.8047 - val_loss: 0.9281 - val_accuracy: 0.8104\n",
      "Epoch 17/100\n",
      "190/236 [=======================>......] - ETA: 0s - loss: 0.9163 - accuracy: 0.8094\n",
      "Epoch 00017: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.9178 - accuracy: 0.8084 - val_loss: 0.9025 - val_accuracy: 0.8139\n",
      "Epoch 18/100\n",
      "190/236 [=======================>......] - ETA: 0s - loss: 0.8956 - accuracy: 0.8132\n",
      "Epoch 00018: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8931 - accuracy: 0.8128 - val_loss: 0.8791 - val_accuracy: 0.8184\n",
      "Epoch 19/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.8737 - accuracy: 0.8170\n",
      "Epoch 00019: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8707 - accuracy: 0.8168 - val_loss: 0.8577 - val_accuracy: 0.8221\n",
      "Epoch 20/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.8503 - accuracy: 0.8199\n",
      "Epoch 00020: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.8500 - accuracy: 0.8201 - val_loss: 0.8381 - val_accuracy: 0.8250\n",
      "Epoch 21/100\n",
      "217/236 [==========================>...] - ETA: 0s - loss: 0.8295 - accuracy: 0.8232\n",
      "Epoch 00021: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.8310 - accuracy: 0.8230 - val_loss: 0.8199 - val_accuracy: 0.8291\n",
      "Epoch 22/100\n",
      "196/236 [=======================>......] - ETA: 0s - loss: 0.8170 - accuracy: 0.8259\n",
      "Epoch 00022: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8135 - accuracy: 0.8265 - val_loss: 0.8032 - val_accuracy: 0.8308\n",
      "Epoch 23/100\n",
      "189/236 [=======================>......] - ETA: 0s - loss: 0.7992 - accuracy: 0.8281\n",
      "Epoch 00023: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7972 - accuracy: 0.8289 - val_loss: 0.7876 - val_accuracy: 0.8332\n",
      "Epoch 24/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.7844 - accuracy: 0.8298\n",
      "Epoch 00024: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7821 - accuracy: 0.8313 - val_loss: 0.7732 - val_accuracy: 0.8355\n",
      "Epoch 25/100\n",
      "211/236 [=========================>....] - ETA: 0s - loss: 0.7706 - accuracy: 0.8326\n",
      "Epoch 00025: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.7680 - accuracy: 0.8336 - val_loss: 0.7596 - val_accuracy: 0.8379\n",
      "Epoch 26/100\n",
      "217/236 [==========================>...] - ETA: 0s - loss: 0.7569 - accuracy: 0.8338\n",
      "Epoch 00026: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.7548 - accuracy: 0.8354 - val_loss: 0.7470 - val_accuracy: 0.8395\n",
      "Epoch 27/100\n",
      "220/236 [==========================>...] - ETA: 0s - loss: 0.7430 - accuracy: 0.8365\n",
      "Epoch 00027: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7424 - accuracy: 0.8371 - val_loss: 0.7351 - val_accuracy: 0.8405\n",
      "Epoch 28/100\n",
      "208/236 [=========================>....] - ETA: 0s - loss: 0.7312 - accuracy: 0.8384\n",
      "Epoch 00028: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.7308 - accuracy: 0.8392 - val_loss: 0.7240 - val_accuracy: 0.8406\n",
      "Epoch 29/100\n",
      "225/236 [===========================>..] - ETA: 0s - loss: 0.7197 - accuracy: 0.8404\n",
      "Epoch 00029: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7199 - accuracy: 0.8404 - val_loss: 0.7135 - val_accuracy: 0.8422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 0.7067 - accuracy: 0.8435\n",
      "Epoch 00030: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7096 - accuracy: 0.8420 - val_loss: 0.7036 - val_accuracy: 0.8440\n",
      "Epoch 31/100\n",
      "218/236 [==========================>...] - ETA: 0s - loss: 0.6994 - accuracy: 0.8442\n",
      "Epoch 00031: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6998 - accuracy: 0.8434 - val_loss: 0.6943 - val_accuracy: 0.8447\n",
      "Epoch 32/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 0.6916 - accuracy: 0.8454\n",
      "Epoch 00032: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6906 - accuracy: 0.8452 - val_loss: 0.6854 - val_accuracy: 0.8459\n",
      "Epoch 33/100\n",
      "201/236 [========================>.....] - ETA: 0s - loss: 0.6828 - accuracy: 0.8465\n",
      "Epoch 00033: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6819 - accuracy: 0.8466 - val_loss: 0.6770 - val_accuracy: 0.8480\n",
      "Epoch 34/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.6732 - accuracy: 0.8480\n",
      "Epoch 00034: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6736 - accuracy: 0.8477 - val_loss: 0.6690 - val_accuracy: 0.8490\n",
      "Epoch 35/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.6672 - accuracy: 0.8493\n",
      "Epoch 00035: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6656 - accuracy: 0.8490 - val_loss: 0.6614 - val_accuracy: 0.8503\n",
      "Epoch 36/100\n",
      "216/236 [==========================>...] - ETA: 0s - loss: 0.6583 - accuracy: 0.8499\n",
      "Epoch 00036: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6581 - accuracy: 0.8503 - val_loss: 0.6542 - val_accuracy: 0.8517\n",
      "Epoch 37/100\n",
      "225/236 [===========================>..] - ETA: 0s - loss: 0.6519 - accuracy: 0.8508\n",
      "Epoch 00037: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6510 - accuracy: 0.8514 - val_loss: 0.6473 - val_accuracy: 0.8526\n",
      "Epoch 38/100\n",
      "202/236 [========================>.....] - ETA: 0s - loss: 0.6466 - accuracy: 0.8513\n",
      "Epoch 00038: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6441 - accuracy: 0.8522 - val_loss: 0.6407 - val_accuracy: 0.8541\n",
      "Epoch 39/100\n",
      "214/236 [==========================>...] - ETA: 0s - loss: 0.6395 - accuracy: 0.8529\n",
      "Epoch 00039: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.8529 - val_loss: 0.6344 - val_accuracy: 0.8546\n",
      "Epoch 40/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.6310 - accuracy: 0.8541\n",
      "Epoch 00040: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6313 - accuracy: 0.8539 - val_loss: 0.6284 - val_accuracy: 0.8558\n",
      "Epoch 41/100\n",
      "228/236 [===========================>..] - ETA: 0s - loss: 0.6261 - accuracy: 0.8544\n",
      "Epoch 00041: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6253 - accuracy: 0.8547 - val_loss: 0.6226 - val_accuracy: 0.8566\n",
      "Epoch 42/100\n",
      "224/236 [===========================>..] - ETA: 0s - loss: 0.6192 - accuracy: 0.8552\n",
      "Epoch 00042: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6195 - accuracy: 0.8551 - val_loss: 0.6170 - val_accuracy: 0.8577\n",
      "Epoch 43/100\n",
      "228/236 [===========================>..] - ETA: 0s - loss: 0.6139 - accuracy: 0.8562\n",
      "Epoch 00043: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6140 - accuracy: 0.8559 - val_loss: 0.6117 - val_accuracy: 0.8590\n",
      "Epoch 44/100\n",
      "216/236 [==========================>...] - ETA: 0s - loss: 0.6075 - accuracy: 0.8575\n",
      "Epoch 00044: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.8566 - val_loss: 0.6066 - val_accuracy: 0.8590\n",
      "Epoch 45/100\n",
      "217/236 [==========================>...] - ETA: 0s - loss: 0.6054 - accuracy: 0.8570\n",
      "Epoch 00045: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6035 - accuracy: 0.8571 - val_loss: 0.6017 - val_accuracy: 0.8600\n",
      "Epoch 46/100\n",
      "214/236 [==========================>...] - ETA: 0s - loss: 0.5998 - accuracy: 0.8571\n",
      "Epoch 00046: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5986 - accuracy: 0.8581 - val_loss: 0.5969 - val_accuracy: 0.8605\n",
      "Epoch 47/100\n",
      "187/236 [======================>.......] - ETA: 0s - loss: 0.5969 - accuracy: 0.8574\n",
      "Epoch 00047: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5938 - accuracy: 0.8588 - val_loss: 0.5924 - val_accuracy: 0.8609\n",
      "Epoch 48/100\n",
      "209/236 [=========================>....] - ETA: 0s - loss: 0.5872 - accuracy: 0.8604\n",
      "Epoch 00048: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.8597 - val_loss: 0.5879 - val_accuracy: 0.8617\n",
      "Epoch 49/100\n",
      "209/236 [=========================>....] - ETA: 0s - loss: 0.5849 - accuracy: 0.8608\n",
      "Epoch 00049: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.8607 - val_loss: 0.5837 - val_accuracy: 0.8628\n",
      "Epoch 50/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.5868 - accuracy: 0.8588\n",
      "Epoch 00050: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5805 - accuracy: 0.8611 - val_loss: 0.5796 - val_accuracy: 0.8634\n",
      "Epoch 51/100\n",
      "227/236 [===========================>..] - ETA: 0s - loss: 0.5758 - accuracy: 0.8624\n",
      "Epoch 00051: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5764 - accuracy: 0.8618 - val_loss: 0.5756 - val_accuracy: 0.8641\n",
      "Epoch 52/100\n",
      "222/236 [===========================>..] - ETA: 0s - loss: 0.5730 - accuracy: 0.8626\n",
      "Epoch 00052: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.8623 - val_loss: 0.5718 - val_accuracy: 0.8653\n",
      "Epoch 53/100\n",
      "215/236 [==========================>...] - ETA: 0s - loss: 0.5698 - accuracy: 0.8625\n",
      "Epoch 00053: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.8631 - val_loss: 0.5681 - val_accuracy: 0.8658\n",
      "Epoch 54/100\n",
      "204/236 [========================>.....] - ETA: 0s - loss: 0.5627 - accuracy: 0.8640\n",
      "Epoch 00054: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.8639 - val_loss: 0.5645 - val_accuracy: 0.8656\n",
      "Epoch 55/100\n",
      "226/236 [===========================>..] - ETA: 0s - loss: 0.5617 - accuracy: 0.8645\n",
      "Epoch 00055: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5612 - accuracy: 0.8643 - val_loss: 0.5610 - val_accuracy: 0.8665\n",
      "Epoch 56/100\n",
      "217/236 [==========================>...] - ETA: 0s - loss: 0.5587 - accuracy: 0.8646\n",
      "Epoch 00056: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.8650 - val_loss: 0.5576 - val_accuracy: 0.8668\n",
      "Epoch 57/100\n",
      "202/236 [========================>.....] - ETA: 0s - loss: 0.5531 - accuracy: 0.8662\n",
      "Epoch 00057: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5543 - accuracy: 0.8652 - val_loss: 0.5543 - val_accuracy: 0.8673\n",
      "Epoch 58/100\n",
      "191/236 [=======================>......] - ETA: 0s - loss: 0.5474 - accuracy: 0.8671\n",
      "Epoch 00058: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5510 - accuracy: 0.8656 - val_loss: 0.5512 - val_accuracy: 0.8679\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/236 [==========================>...] - ETA: 0s - loss: 0.5477 - accuracy: 0.8664\n",
      "Epoch 00059: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.8658 - val_loss: 0.5481 - val_accuracy: 0.8679\n",
      "Epoch 60/100\n",
      "228/236 [===========================>..] - ETA: 0s - loss: 0.5443 - accuracy: 0.8665\n",
      "Epoch 00060: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5446 - accuracy: 0.8662 - val_loss: 0.5451 - val_accuracy: 0.8687\n",
      "Epoch 61/100\n",
      "201/236 [========================>.....] - ETA: 0s - loss: 0.5417 - accuracy: 0.8665\n",
      "Epoch 00061: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.8668 - val_loss: 0.5422 - val_accuracy: 0.8692\n",
      "Epoch 62/100\n",
      "211/236 [=========================>....] - ETA: 0s - loss: 0.5398 - accuracy: 0.8671 ETA: 0s - loss: 0.5466 - accuracy: 0.\n",
      "Epoch 00062: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5386 - accuracy: 0.8673 - val_loss: 0.5393 - val_accuracy: 0.8694\n",
      "Epoch 63/100\n",
      "222/236 [===========================>..] - ETA: 0s - loss: 0.5363 - accuracy: 0.8677\n",
      "Epoch 00063: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5357 - accuracy: 0.8679 - val_loss: 0.5366 - val_accuracy: 0.8697\n",
      "Epoch 64/100\n",
      "214/236 [==========================>...] - ETA: 0s - loss: 0.5330 - accuracy: 0.8679\n",
      "Epoch 00064: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.8683 - val_loss: 0.5339 - val_accuracy: 0.8701\n",
      "Epoch 65/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.5279 - accuracy: 0.8704\n",
      "Epoch 00065: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.8692 - val_loss: 0.5313 - val_accuracy: 0.8702\n",
      "Epoch 66/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.5266 - accuracy: 0.8697\n",
      "Epoch 00066: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.8696 - val_loss: 0.5287 - val_accuracy: 0.8706\n",
      "Epoch 67/100\n",
      "221/236 [===========================>..] - ETA: 0s - loss: 0.5256 - accuracy: 0.8700\n",
      "Epoch 00067: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.8702 - val_loss: 0.5262 - val_accuracy: 0.8707\n",
      "Epoch 68/100\n",
      "204/236 [========================>.....] - ETA: 0s - loss: 0.5239 - accuracy: 0.8707\n",
      "Epoch 00068: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.8705 - val_loss: 0.5238 - val_accuracy: 0.8709\n",
      "Epoch 69/100\n",
      "223/236 [===========================>..] - ETA: 0s - loss: 0.5216 - accuracy: 0.8709\n",
      "Epoch 00069: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.8710 - val_loss: 0.5214 - val_accuracy: 0.8714\n",
      "Epoch 70/100\n",
      "196/236 [=======================>......] - ETA: 0s - loss: 0.5166 - accuracy: 0.8718\n",
      "Epoch 00070: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5176 - accuracy: 0.8715 - val_loss: 0.5191 - val_accuracy: 0.8716\n",
      "Epoch 71/100\n",
      "199/236 [========================>.....] - ETA: 0s - loss: 0.5179 - accuracy: 0.8706\n",
      "Epoch 00071: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5152 - accuracy: 0.8717 - val_loss: 0.5169 - val_accuracy: 0.8718\n",
      "Epoch 72/100\n",
      "193/236 [=======================>......] - ETA: 0s - loss: 0.5140 - accuracy: 0.8708\n",
      "Epoch 00072: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.8722 - val_loss: 0.5147 - val_accuracy: 0.8719\n",
      "Epoch 73/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.5093 - accuracy: 0.8732\n",
      "Epoch 00073: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5107 - accuracy: 0.8730 - val_loss: 0.5126 - val_accuracy: 0.8730\n",
      "Epoch 74/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.5098 - accuracy: 0.8728\n",
      "Epoch 00074: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5085 - accuracy: 0.8731 - val_loss: 0.5105 - val_accuracy: 0.8735\n",
      "Epoch 75/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.5075 - accuracy: 0.8729\n",
      "Epoch 00075: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5063 - accuracy: 0.8734 - val_loss: 0.5084 - val_accuracy: 0.8736\n",
      "Epoch 76/100\n",
      "199/236 [========================>.....] - ETA: 0s - loss: 0.5044 - accuracy: 0.8740\n",
      "Epoch 00076: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5042 - accuracy: 0.8738 - val_loss: 0.5064 - val_accuracy: 0.8740\n",
      "Epoch 77/100\n",
      "200/236 [========================>.....] - ETA: 0s - loss: 0.5005 - accuracy: 0.8750\n",
      "Epoch 00077: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5022 - accuracy: 0.8742 - val_loss: 0.5044 - val_accuracy: 0.8743\n",
      "Epoch 78/100\n",
      "195/236 [=======================>......] - ETA: 0s - loss: 0.5005 - accuracy: 0.8741\n",
      "Epoch 00078: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5002 - accuracy: 0.8749 - val_loss: 0.5025 - val_accuracy: 0.8745\n",
      "Epoch 79/100\n",
      "200/236 [========================>.....] - ETA: 0s - loss: 0.4964 - accuracy: 0.8752\n",
      "Epoch 00079: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4982 - accuracy: 0.8750 - val_loss: 0.5006 - val_accuracy: 0.8755\n",
      "Epoch 80/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.4972 - accuracy: 0.8758\n",
      "Epoch 00080: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4963 - accuracy: 0.8756 - val_loss: 0.4988 - val_accuracy: 0.8757\n",
      "Epoch 81/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.4973 - accuracy: 0.8754\n",
      "Epoch 00081: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4944 - accuracy: 0.8757 - val_loss: 0.4970 - val_accuracy: 0.8757\n",
      "Epoch 82/100\n",
      "200/236 [========================>.....] - ETA: 0s - loss: 0.4930 - accuracy: 0.8768\n",
      "Epoch 00082: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4926 - accuracy: 0.8760 - val_loss: 0.4952 - val_accuracy: 0.8759\n",
      "Epoch 83/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.4937 - accuracy: 0.8751\n",
      "Epoch 00083: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4908 - accuracy: 0.8763 - val_loss: 0.4935 - val_accuracy: 0.8760\n",
      "Epoch 84/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.4922 - accuracy: 0.8759\n",
      "Epoch 00084: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4890 - accuracy: 0.8765 - val_loss: 0.4918 - val_accuracy: 0.8765\n",
      "Epoch 85/100\n",
      "203/236 [========================>.....] - ETA: 0s - loss: 0.4864 - accuracy: 0.8769\n",
      "Epoch 00085: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4873 - accuracy: 0.8770 - val_loss: 0.4901 - val_accuracy: 0.8767\n",
      "Epoch 86/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.4827 - accuracy: 0.8777\n",
      "Epoch 00086: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4856 - accuracy: 0.8773 - val_loss: 0.4885 - val_accuracy: 0.8764\n",
      "Epoch 87/100\n",
      "197/236 [========================>.....] - ETA: 0s - loss: 0.4799 - accuracy: 0.8789\n",
      "Epoch 00087: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.8775 - val_loss: 0.4869 - val_accuracy: 0.8767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.4793 - accuracy: 0.8789\n",
      "Epoch 00088: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4822 - accuracy: 0.8776 - val_loss: 0.4853 - val_accuracy: 0.8769\n",
      "Epoch 89/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.4803 - accuracy: 0.8782\n",
      "Epoch 00089: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4806 - accuracy: 0.8781 - val_loss: 0.4838 - val_accuracy: 0.8767\n",
      "Epoch 90/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.4792 - accuracy: 0.8783\n",
      "Epoch 00090: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4791 - accuracy: 0.8783 - val_loss: 0.4823 - val_accuracy: 0.8767\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.8784\n",
      "Epoch 00091: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.8784 - val_loss: 0.4808 - val_accuracy: 0.8772\n",
      "Epoch 92/100\n",
      "219/236 [==========================>...] - ETA: 0s - loss: 0.4744 - accuracy: 0.8791\n",
      "Epoch 00092: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4760 - accuracy: 0.8784 - val_loss: 0.4794 - val_accuracy: 0.8777\n",
      "Epoch 93/100\n",
      "226/236 [===========================>..] - ETA: 0s - loss: 0.4742 - accuracy: 0.8790\n",
      "Epoch 00093: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4745 - accuracy: 0.8789 - val_loss: 0.4779 - val_accuracy: 0.8779\n",
      "Epoch 94/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.4731 - accuracy: 0.8791\n",
      "Epoch 00094: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4730 - accuracy: 0.8791 - val_loss: 0.4765 - val_accuracy: 0.8781\n",
      "Epoch 95/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.4719 - accuracy: 0.8790\n",
      "Epoch 00095: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4716 - accuracy: 0.8794 - val_loss: 0.4752 - val_accuracy: 0.8784\n",
      "Epoch 96/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.4711 - accuracy: 0.8785\n",
      "Epoch 00096: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4702 - accuracy: 0.8794 - val_loss: 0.4738 - val_accuracy: 0.8786\n",
      "Epoch 97/100\n",
      "198/236 [========================>.....] - ETA: 0s - loss: 0.4672 - accuracy: 0.8805\n",
      "Epoch 00097: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4688 - accuracy: 0.8794 - val_loss: 0.4725 - val_accuracy: 0.8786\n",
      "Epoch 98/100\n",
      "226/236 [===========================>..] - ETA: 0s - loss: 0.4676 - accuracy: 0.8798\n",
      "Epoch 00098: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.8798 - val_loss: 0.4712 - val_accuracy: 0.8786\n",
      "Epoch 99/100\n",
      "216/236 [==========================>...] - ETA: 0s - loss: 0.4655 - accuracy: 0.8803\n",
      "Epoch 00099: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.8798 - val_loss: 0.4699 - val_accuracy: 0.8789\n",
      "Epoch 100/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.4641 - accuracy: 0.8802\n",
      "Epoch 00100: saving model to ./training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4647 - accuracy: 0.8799 - val_loss: 0.4686 - val_accuracy: 0.8789\n",
      "394/394 [==============================] - 0s 746us/step - loss: 0.4813 - accuracy: 0.8735\n",
      "[0.48125016689300537, 0.8734920620918274]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # keras model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Flatten(Input Layer)\n",
    "                                                    # Dense(Output Layer)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "\n",
    "# Data Split\n",
    "# 기존에는 test_x_data, test_t_data 이 두 데이터를 validation 용도로 \n",
    "# 사용했어요!\n",
    "# 이제는 test_x_data, test_t_data 이 두 데이터를 test 용도로 사용할꺼예요!\n",
    "# 최종 모델 성능평가를 위해서 딱 1번만 사용할꺼예요!\n",
    "# 그러면 validation은 어떻게 하나요?\n",
    "# keras는 학습할 때 train data를 일정부분 나누어서 자체 validation이 가능\n",
    "# keras 기능을 이용해서 validation 처리\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n",
    "\n",
    "# 우리는 loss 지정할 때 sparse_categorical_crossentropy로 loss함수를\n",
    "# 지정할 예정이기 때문에 label에 대한 one-hot encoding처리가 필요 없어요!\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()\n",
    "\n",
    "# layer 추가\n",
    "# input layer\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model의 저장할려고 해요. model 구조 빼고 checkpoint기능을 이용해서 \n",
    "# weight, b만 저장\n",
    "# 어디에 저장할지를 알려줘야 해요!\n",
    "checkpoint_path = './training_ckpt/cp.ckpt'\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)  # 실제 경로로 만들어요!\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "\n",
    "\n",
    "# 학습결과를 변수에 저장\n",
    "history = model.fit(norm_train_x_data,\n",
    "                    train_t_data,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "# 우리 모델에 대한 최종 평가진행\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#        loss               accuracy\n",
    "# [0.4799305200576782, 0.876031756401062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5c2edd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "394/394 [==============================] - 0s 665us/step - loss: 2.4074 - accuracy: 0.1291\n",
      "[2.4074270725250244, 0.12912698090076447]\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "# 아하 ... 이렇게 저장할 수 있네요!!\n",
    "# 불러서 다시 사용하려면 어떻게 해야 하나요?\n",
    "\n",
    "# 확인하기 위해...\n",
    "# 일단 학습하지 않은 상태로 evaluation을 진행하면 당연히 평가결과가 \n",
    "# 좋지 않겠죠??? 이거 확인하고\n",
    "# 그 다음에 ckeckpoint 파일을 로드해서 model을 재 설정하고 평가를 진행\n",
    "# 좋게 나오겠네요!!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # keras model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Flatten(Input Layer)\n",
    "                                                    # Dense(Output Layer)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "\n",
    "# Data Split\n",
    "# 기존에는 test_x_data, test_t_data 이 두 데이터를 validation 용도로 \n",
    "# 사용했어요!\n",
    "# 이제는 test_x_data, test_t_data 이 두 데이터를 test 용도로 사용할꺼예요!\n",
    "# 최종 모델 성능평가를 위해서 딱 1번만 사용할꺼예요!\n",
    "# 그러면 validation은 어떻게 하나요?\n",
    "# keras는 학습할 때 train data를 일정부분 나누어서 자체 validation이 가능\n",
    "# keras 기능을 이용해서 validation 처리\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n",
    "\n",
    "# 우리는 loss 지정할 때 sparse_categorical_crossentropy로 loss함수를\n",
    "# 지정할 예정이기 때문에 label에 대한 one-hot encoding처리가 필요 없어요!\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()\n",
    "\n",
    "# layer 추가\n",
    "# input layer\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 원래는 학습을 진행행해야 해요! 그런데 학습을 안할꺼예요!\n",
    "\n",
    "# 학습을 진행하지 않고 최종 평가진행\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#        loss               accuracy\n",
    "# [2.4244349002838135, 0.08492063730955124]\n",
    "# 당연히 학습이 안된 모델이기 때문에 이렇게 나오는게 정상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "211f7c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/394 [..............................] - ETA: 0s - loss: 0.6143 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "394/394 [==============================] - 0s 827us/step - loss: 0.4813 - accuracy: 0.8735\n",
      "[0.48125016689300537, 0.8734920620918274]\n"
     ]
    }
   ],
   "source": [
    "# 이번에는 checkpoint 파일에 있는 weight를 load한 후\n",
    "# evaluation 시켜보아요!\n",
    "\n",
    "checkpoint_path = './training_ckpt/cp.ckpt'\n",
    "model.load_weights(checkpoint_path)\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "# [0.48125016689300537, 0.8734920620918274]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_TF2] *",
   "language": "python",
   "name": "conda-env-machine_TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
